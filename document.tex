\documentclass[jou,a4paper,notxfonts]{apa}
\usepackage{graphicx} 
\usepackage{palatino}
\usepackage{fancyhdr}
\pagestyle{fancy}

% header on first page
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyhead[L]{\small
Journal of Eye Movement Research\\
1(1):1, 1-2}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}



\title{Interacting with Objects in the Environment by Gaze and Hand Gestures} %Fill this in

\threeauthors{Author 1}{Author 2}{Author 3}
\threeaffiliations{University of Queensland}{IT University of Copenhagen}{CSIRO ICT Centre}

\journal{}
\volume{}


\abstract{Head-mounted gaze trackers can be used for gaze estimation purposes in mobile scenarios. A wireless gaze
tracker in the form of gaze tracking glasses permits the continuous monitoring of a subject's point of regard on the
surrounding 3D environment. In this work, we combine gaze tracking and hand gesture recognition to allow a subject to
interact with objects in the environment by gazing at them, pointing part, and generating a hand gesture command,
control part. We use low-cost hardware to build a set of gaze tracking glasses consisting of glasses frame, eye tracking
camera and field of view camera. We also use a binary beacon recognition library to identify objects in the environment,
a hand gesture classification algorithm to generate control commands and an open source gaze estimation algorithm to
obtain the user's gaze coordinates. When combining all these elements, the emerging system permits a subject to move
freely in an environment, positions his gaze in an object he want to interact with, and transmit a control command by
performing a hand gesture. The specific object is identified by the binary beacon visible in its surface. This
innovative HCI paradigm opens up new forms of interaction with objects in an smart environment.
% 200 word abstract
\linebreak \linebreak {\bf Keywords: Gaze Tracking Glasses, Mobile Interaction, Gaze Interaction, HCI}}
\acknowledgements{}

\shorttitle{Interacting with Objects in the Environment by Gaze and Hand Gestures}
\rightheader{}

% repeat the authors here (use et al if more than three authors):
\leftheader{Author 1, Author 2, Author 3}




\begin{document}
\maketitle%\begin{center}

\lhead{\small
Journal of Eye Movement Research\\
1(1):1, 1-2
}
\rhead{
One, A., Two, A., & Three, A. (2013)\\
Interacting with Objects in the Environment by Gaze and Hand Gestures
}
\thispagestyle{plain}

\section{Introduction} 
% We should clearify that this paper does not contribute in the hand gesture recognition but it uses the gesture and
% gaze in a mobile situation for controlling a robot.
Body language and gaze are important forms of communication among humans. In this work we present a system that combines
gaze pointing and hand gestures to interact with objects in the environment. Our system merges a video-based gaze 
tracker, a hand gesture classifier and a visual marker recognition module into an innovate HCI device that permits novel 
forms of interaction with the environment. Gaze is used as a pointing mechanism to select the object with which the 
subject want to interact. A visual binary marker in the surface of the object is used for identification purposes by
the system. Finally, a hand gesture is mapped to a specific control command that makes the object carry out a particular
function.

Eye tracking refers to the monitoring of eye movements. Gaze tracking is the process of measuring the point of gaze
(where one is looking). A head mounted eye tracker is a device that permits measuring eye movement while the subject is
moving around an environment. Eye trackers are used in research on the visual system, in psychology, in cognitive
linguistics and in product design. There are a number of methods for measuring eye movement. The most popular variant
uses video images from which the eye position is extracted. Other methods use search coils or are based on the
electrooculogram.

Automatic gesture recognition is a topic in computer science and language technology that strives to interpret human
gestures via computational algorithms. Gestures can originate from any bodily motion or state but commonly originate
from the face or hands. An appealing feature of gestural interfaces is that they make it possible for users to communicate
with objects without need for external control devices. Hand gestures are an obvious choice as a mechanism to interact
with objects in the environment.

Automated hand gesture recognition is challenging since in order for such an approach to represent a serious alternative
to conventional input devices applications based on computer vision should be able to work successfully under
uncontrolled light conditions, no matter what kind of background the user stands in front of. In addition, deformable
and articulated objects like hands represent additional difficulty both for segmentation and shape recognition
purposes.

Visual marker system consists of a set of patterns that can be detected by a computer equipped
with a camera and an appropriate detection algorithm \cite{middel19detection}. Markers placed in the environment
provide easily detectable visual cues for object detection in the environment. In this work, we use the open source
library XXXXXXXXXXXXXXXXX FOR DIAKO TO FILL XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX. This library provides our system with
the ability to recognize specific objects in the environment.


The hand gesture recognition module we develop here is able to detect a hand in front of the scene camera of the gaze
tracking glasses and it is also able to detect the number of fingers that the hand is holding up. We define the gesture
as holding the hand with a preset number of fingers hold up  for a predefined dwell time and moving it in a particular
direction: up, down, left  or right. The hand gesture recognition worked well for natural skin color, but using a latex
glove of a given color improves performance.


Haytham provides real-time gaze estimation in the user’s field of view or in the computer display by analyzing eye movement.
 
The system recognizes objects in the environment by detecting visual markers associated to them through the scene camera.

To interact with an object in the environment, the user of the glasses needs to first look at the binary beacon that
identifies the object and then perform the gesture. In this way, only that particular object in the environment 
will react to the hand gesture.


In Summary, this work represents a proof of concept for an innovative form of interacting with objects in the environment
through gaze and hand gestures. The off-the-shelf components used to build the hardware, the low cost of building the
entire system and the open source nature of the algorithms used, make this form of interaction amenable for spreading
among academic institutions and research labs to further investigate and stretch the possibilities of the innovative
HCI paradigm.

\subsection{Related Work}

%Related work on mobile gaze-based interaction, and hand gesture interaction with robot
\subsection{Head mounted eye trackers}
\subsection{Gaze for Interaction}
\subsection{Gestures in HCI}
\subsection{Combining gaze and gesture}
%methods and limitations
\subsubsection{Gesture for Interaction}
%
\subsubsection{Gaze enhanced interaction }
%Pehaps different methods or modes that has been studied in our case

\section{Method}

\subsection{Gaze Tracking}
Eyes are used by humans to obtain information about the surroundings and to communicate information. When something
attracts our attention, we position our gaze on it, thus performing a \textit{fixation}. A fixation usually has a
duration of at least 150 milliseconds (ms). The fast eye movements that occur between fixations are known as
\textit{saccades}, and they are used to reposition the eye so that the object of interest is projected onto the fovea.
The direction of gaze thus reflects the focus of \textit{attention} and also provides an indirect hint for
\textit{intention} \cite{velichkovsky}.


A video-based gaze tracking system seeks to find where a person is looking, i.e. the Point of Regard (PoR), using images
obtained from the eye by one or more cameras. Most systems employ infrared illumination that is invisible to the human
eye and hence it is not distracting for the user. Infrared light improves image contrast and produces a reflection on
the cornea, known as corneal reflection or glint. Eye features such as the corneal reflections and the center of the
pupil/iris can be used to estimate the PoR. Figure \ref{screenGazeTracker} shows a screenshot of an eye being tracked by
the open-source ITU Gaze Tracker \cite{lowcostitugazetracker,Rozado2012}. In this case, the center of the pupil and two
corneal reflections are the features being tracked.

%How to insert a figure (image):
\begin{figure}[tp]
 \fitbitmap{figures/screenGazeTracker.jpg}
 \caption{\textbf{The Open Source ITU Gaze Tracker Tracking One Eye.} The
features tracked in the image are the pupil center and two corneal reflections. These features are used by the gaze 
estimation algorithms to determinine the PoR of the user on the screen.}
 % the label is used to put refrences to this figure in the text
 \label{screenGazeTracker}
\end{figure}

Depending on the hardware configuration of the different components, gaze tracking systems can be classified as either
\textit{remote} or \textit{head-mounted}. In remote systems, the camera and the light sources are detached from the
user, normally around the device screen, whereas in head-mounted systems the components are placed on the user's head, usually
mounted on a helmet or a pair of safety glasses \cite{lowcostitugazetracker}.

Head-mounted eye trackers can be used for mobile interaction as well as gaze estimation purposes.
In this work, we have build a low-cost headmounted eye tracker using off-the-shelf components.  The system consists of
safety glasses, batteries, wireless eye camera and the wireless scene or field of view camera. The wireless eye camera
and the associated software monitors the position of the pupil on the image and an infrared reflection in the iris. The
vector difference between the pupil center and the glint reflection in the iris can be tracked during a calibration
procedure to build a model of the user's gaze.

The calibration procedure consists on the user looking at a number of points on the environment and marking them on
the scene camera video stream while the user fixates on them.  Once a calibration procedure is completed, the gaze
estimation algorithm is able to determining the point of regard of the user in the environment.

We use the open-source Haytham \cite{Mardanbegi2011} gaze tracker developed by Diako Mardanbegui at the IT University of
Copenhagen for mobile gaze estimation and extended it with a hand gesture recognition module that enables specific
interaction with objects in the environment through gaze and hand gestures.

\subsection{Constructing the eye tracking glasses}

Materials:
Plastic safety glasses
Wireless camera with infra-red LEDs (eye camera)
Wireless camera	(scene camera)
Tin strips
Araldite
Steel wire
Tape
Double sided tape
9V battery
Tin snips

1. An area was traced onto the lens of the glasses where the eyes will be approximately when the user puts on the safety googles. 
2. Using the tin snips, the plastic part of the lenses bounded by the previously traced area was cut away. It is important that the
majority of the structure of the glasses is left intact to allow for stability.
3. Tin was cut to the size and shape of the infrared camera using the tin snips.
4. Steel wire was cut to a size of 25cm.
5. The steel wire was then attached to piece of tin using araldite.
6. Using double sided tape, the tin was attached to the back of the camera.
7. The steel wire was then bent into an 'L' shape and attached to the right side of the glasses (frame) using tape.
8. The wires for the battery were extended and the 9V battery was attached to the left side of the glasses to ensure that the glasses
are not unevenly balanced.
8. Using the haytham software, the position of the camera was checked to ensure the camera feed is filming the entire eye.
It was found that the best position of the eye is below the glasses so it doesn't obstruct the user's vision. 
8. The scene camera was mounted to the right of side of the glasses using sticky tape.



% ---------------------------------------------------------------
\subsection{Gaze Tracking Glasses}
%Maybe Diako should take care of this subsection

\subsection{Visual Markers Recognition}
\subsection{Hand Gesture Recognition}
\subsection{Putting it all together}


\section{Application}
Point to demo video: \url{http://www.youtube.com/watch?v=tBOfvZboGfk}


%---------------------------------------------------------------
\section{Discussion}

For demonstration purposes, in the associated video \cite{gazetrackinglassesandhandgesturerecognitionVideo}, the system
presented here is used to interact with a computer, a breadboard with a set of light emitting diodes and with a robot


In this work we have shown how to interact with objects in the environment through an innovative combination of
gaze and hand gestures. The method is easily extendable to multiple objects in the environment and a wide array of hand
gestures. The low-cost headmounted eye tracker does not compensate for parallax error, inability to differentiate
between the working plane and the calibration plane, and these can limit interaction with objects at a distance and
objects up close. We have tested the system  for interacting with a computer,  a breadboard and a mobile robot.
This work has demonstrated the feasibility of the approach as well as its usefulness.


-Problems with skin detection vs Gloves
-Problems through lack of visual feedback (audio feedback as a substitute)


\bibliography{library}


\end{document}